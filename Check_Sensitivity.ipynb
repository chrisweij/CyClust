{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "919a1485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found ./ei.ans.land-sea.nc\n",
      "Found ./ei.ans.orog.nc\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- encoding: utf-8\n",
    "\n",
    "#from geopy.distance import great_circle as great_circle_old\n",
    "from datetime import datetime as dt, timedelta as td\n",
    "import numpy as np\n",
    "from numpy import loadtxt\n",
    "\n",
    "import sparse\n",
    "from scipy.sparse import dok_matrix\n",
    "import time\n",
    "from timeit import default_timer as timer\n",
    "import yaml\n",
    "#from Cluster_functions import read_file, read_file_clim, dt_array\n",
    "\n",
    "with open(\"Options.yaml\") as f:\n",
    "    Options = yaml.safe_load(f)\n",
    "\n",
    "#Specific functions related to the clustering algorithm\n",
    "from Cluster_functions import *\n",
    "from dynlib.metio.erainterim import conf, dt, get_instantaneous, metsave, metopen\n",
    "\n",
    "#Plotting related\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import dynlib.proj as proj\n",
    "import dynlib.figures as fig\n",
    "import copy\n",
    "from cmocean import cm as cmoc\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "import os\n",
    "\n",
    "clustchar = \"nolength\" #length\" #\"All\" #\"length\"\"nolength\" or \"all\"\n",
    "minstorms = 2\n",
    "distchar = \"400km\" #\"250km\"\n",
    "calcDensityEI = False\n",
    "\n",
    "#Output file format\n",
    "formatter =  \"{:1.1f}\"\n",
    "outfile_suffix = \"_\" + distchar + \"_\" +\\\n",
    "            Options[\"str_result\"]  + formatter.format( Options[\"distthresh\"]) + \"_tim_\" + formatter.format( Options[\"timthresh\"]) + \"_length_\" + formatter.format( Options[\"lngthresh\"    ]) +\\\n",
    "    \"_timlength_\" + formatter.format( Options[\"timlngthresh\"]*6.0) + \"_\" + Options[\"distmeth\"] + \"_minstorms_\" + str(minstorms) + \"_connect_\"\n",
    "\n",
    "# PLOTTING SETTINGS \"\n",
    "scale_density = np.arange(3,25,1)\n",
    "scale_anomaly = np.arange(-5,5.1,1)\n",
    "scale_anomaly_perc = np.arange(-40,40,5)\n",
    "cmap_IMILAST = (mpl.colors.ListedColormap([\"yellow\",\"orange\",\"green\",\"lightblue\",\"blue\",\"darkblue\",\"midnightblue\",\"purple\",\"red\"])\n",
    "        .with_extremes(over='grey', under='white'))\n",
    "scale_IMILAST = [2,5,10,15,20,25,35,50,70,100]\n",
    "scale_clust = [0.5,1.0,2.0,3.0,4.0,6.0,9.0,12.0,15.0,20.0]\n",
    "colors_clust = [\"white\",\"yellow\",\"orange\",\"green\",\"dodgerblue\",\"red\",\"grey\"]\n",
    "norm = mpl.colors.BoundaryNorm(scale_IMILAST, cmap_IMILAST.N)\n",
    "colors_IMILAST=[\"white\",\"yellow\",\"orange\",\"green\",\"deepskyblue\",\"dodgerblue\",\"royalblue\",\"midnightblue\",\"purple\",\"red\",\"grey\"]\n",
    "\n",
    "def n_hemisphere_new():\n",
    "        ''' Stereographic map, centered on the north pole, covering most of the northern hemisphere\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Basemap\n",
    "                map projection instance\n",
    "        '''\n",
    "\n",
    "        return Basemap(projection='npstere',boundinglat=27.5,lon_0=0,resolution='c', area_thresh=50000)\n",
    "\n",
    "def s_hemisphere_new():\n",
    "        ''' Stereographic map, centered on the north pole, covering most of the northern hemisphere\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Basemap\n",
    "                map projection instance\n",
    "        '''\n",
    "\n",
    "        return Basemap(projection='spstere',boundinglat=-27.5,lon_0=0,resolution='c', area_thresh=50000)\n",
    "\n",
    "ncmask, landmask, maskgrid = metopen(\"ei.ans.land-sea\",q=\"lsm\")\n",
    "ncoro, oro, orogrid = metopen(\"ei.ans.orog\",q=\"z\")\n",
    "oro /= 9.81\n",
    "oro = np.roll(oro,360,-1)\n",
    "nrtimes = 90*4*(2014-1979 + 1)\n",
    "\n",
    "figsize = (10,9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00ae080a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'_400km_Global_Results_EI_1.0_tim_36.0_length_1.5_timlength_48.0_AlongTracksDirect_minstorms_2_connect_'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#########################\n",
    "# Thresholds\n",
    "#########################\n",
    "#1. Distance criterium\n",
    "distthresh = 1.0 #1000.0\n",
    "\n",
    "#2. Time criterium\n",
    "timthresh = 60.0\n",
    "\n",
    "#3. Length criterium \n",
    "#lngthresh = 2000.0\n",
    "lngthresh = 1.5 #calc_Rossby_radius(lat=45)*2.0 # 1000.0\n",
    "\n",
    "#Sensitivity ranges for thresholds\n",
    "#New set of experiments\n",
    "timthreshs = np.arange(1,4.1,0.25)*24.0\n",
    "lngthreshs = np.arange(0.6,2.21,0.2)\n",
    "distthreshs = np.arange(0.5,1.51,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b913df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "grid = copy.copy(maskgrid)\n",
    "grid.x = maskgrid.x[::3,::3] + 180.0 #[0:60,::]\n",
    "grid.y = maskgrid.y[::3,::3] #[0:60,::]\n",
    "grid.dx = maskgrid.dx[::3,::3] #[0:60,::]\n",
    "grid.dy = maskgrid.dy[::3,::3] #[0:60,::]\n",
    "grid.oro = maskgrid.oro[::3,::3] #[0:60,::]\n",
    "grid.nx = 240\n",
    "grid.ny = 121 #60\n",
    "conf.gridsize = (121,240) #(60,240)\n",
    "\n",
    "def calc_density():\n",
    "    #Construct grid\n",
    "    import copy\n",
    "    grid = copy.copy(maskgrid)\n",
    "    grid.x = maskgrid.x[::3,::3] + 180.0#[0:60,::]\n",
    "    grid.y = maskgrid.y[::3,::3] #[0:60,::]\n",
    "    grid.dx = maskgrid.dx[::3,::3] #[0:60,::]\n",
    "    grid.dy = maskgrid.dy[::3,::3] #[0:60,::]\n",
    "    grid.oro = maskgrid.oro[::3,::3] #[0:60,::]\n",
    "    grid.nx = 240\n",
    "    grid.ny = 121 #60\n",
    "    conf.gridsize = (121,240) #(60,240)\n",
    "\n",
    "    #Define arrays\n",
    "    lats = np.arange(90,-90.1,-1.5)\n",
    "    lons = np.arange(0,360,1.5)\n",
    "    mean_storms = np.zeros((len(lats),len(lons))) #Cyclone centre density\n",
    "    mean_tracks = np.zeros((len(lats),len(lons))) #Track density\n",
    "    mean_lysis  = np.zeros((len(lats),len(lons))) #Track density\n",
    "    mean_genesis  = np.zeros((len(lats),len(lons))) #Track density\n",
    "\n",
    "    for latidx in range(len(lats)):\n",
    "        print(\" Lat: \" + str(lats[latidx]))\n",
    "        for lonidx in range(len(lons)):\n",
    "            min_lat = np.nanmax([-90.0,lats[latidx] - 1.5])\n",
    "            max_lat = np.nanmin([90.0,lats[latidx] + 1.5])\n",
    "            min_lon = lons[lonidx] - 1.5\n",
    "            max_lon = lons[lonidx] + 1.5\n",
    "            if((min_lon >= 0.0) & (max_lon <= 360.0)):\n",
    "                temp_id = str_id[(str_lat >= min_lat) & (str_lat <= max_lat) & (str_lon >= min_lon) & (str_lon <= max_lon)]\n",
    "                temp_lon = str_lon[(str_lat >= min_lat) & (str_lat <= max_lat) & (str_lon >= min_lon) & (str_lon <= max_lon)]\n",
    "                temp_lat = str_lat[(str_lat >= min_lat) & (str_lat <= max_lat) & (str_lon >= min_lon) & (str_lon <= max_lon)]\n",
    "                areaidxs = (grid.y >=  min_lat) & (grid.y <= max_lat) & (grid.x  >= min_lon) & (grid.x <= max_lon)\n",
    "            elif(min_lon < 0.0):\n",
    "                temp_id = str_id[(str_lat >= min_lat) & (str_lat <= max_lat) &  (str_lon <= max_lon) | (str_lat >= min_lat) & (str_lat <= max_lat) &  (str_lon >= min_lon  + 360.0)]\n",
    "                areaidxs = (grid.y >=  min_lat) & (grid.y <= max_lat)  & (grid.x  <= max_lon) | (grid.y >=  min_lat) & (grid.y <= max_lat) & (grid.x >= min_lon + 360.0) \n",
    "            elif(max_lon > 360.0):\n",
    "                temp_id = str_id[(str_lat >= min_lat) & (str_lat <= max_lat) & (str_lon >= min_lon) | (str_lat >= min_lat) & (str_lat <= max_lat) & (str_lon <= max_lon - 360.0)]\n",
    "                areaidxs = (grid.y >=  min_lat) & (grid.y <= max_lat)  & (grid.x  >= min_lon) | (grid.y >=  min_lat) & (grid.y <= max_lat) & (grid.x <= max_lon - 360.0)\n",
    "            else:\n",
    "                print(\"I should not be here\")\n",
    "            diff_lon = 3.0\n",
    "            #area = np.abs((max_lat - min_lat)*diff_lon)*np.cos((min_lat+max_lat)/2.*np.pi/180.)*111111**2 #\n",
    "            area = np.nansum(np.abs(grid.dy[areaidxs]*grid.dx[areaidxs]))\n",
    "\n",
    "            mean_storms[latidx,lonidx] = len(temp_id)/(area*nrtimes)*10**12\n",
    "            mean_tracks[latidx,lonidx] = len(np.unique(temp_id))/(area*nrtimes)*10**12\n",
    "            \n",
    "    return mean_storms, mean_tracks\n",
    "    \n",
    "#Construct array with datetimes\n",
    "dt_array = []\n",
    "for yidx in range(1979,2017):\n",
    "\n",
    "    # To get year (integer input) from the user\n",
    "    # year = int(input(\"Enter a year: \"))\n",
    "    if ((yidx + 1) % 4) == 0:\n",
    "        leapyear = True\n",
    "        nr_times = 364 #366*4 #(whole year) 364 (just winter)\n",
    "    else:\n",
    "        leapyear = False\n",
    "        nr_times = 360 #365*4 #(whole year) 360 (just winter)\n",
    "\n",
    "    start = dt(yidx, 12, 1, 0) #(just winter)\n",
    "\n",
    "    dt_array_temp = np.array([start + td(hours=i*6) for i in range(nr_times)])\n",
    "    dt_array.extend(dt_array_temp)\n",
    "\n",
    "def calc_density_radius(distchar = \"250km\", dist_thresh=250, connect=False, save=False, outfile=\"Density.npz\"):\n",
    "    #Define arrays\n",
    "    lats = np.arange(90,-90.1,-1.5)\n",
    "    lons = np.arange(0,360,1.5)\n",
    "\n",
    "    ################################\n",
    "    # Get indices of storms \n",
    "    # so that ids_storms[id] gives the ids in the arrays\n",
    "    # str_id, str_lon,.. belonging to that specific storm\n",
    "    #########################\n",
    "    uniq_ids = np.unique(str_id)\n",
    "    ids_storms = get_indices_sparse(str_id)\n",
    "    nrstorms = len(uniq_ids)\n",
    "\n",
    "    #Define arrays\n",
    "    storms = np.zeros((len(dt_array),len(lats),len(lons))) #Cyclone centre density\n",
    "    tracks = np.zeros((len(dt_array),len(lats),len(lons))) #Track density\n",
    "    lysis  = np.zeros((len(dt_array),len(lats),len(lons))) #Track density\n",
    "    genesis  = np.zeros((len(dt_array),len(lats),len(lons))) #Track density\n",
    "    \n",
    "    mean_storms_seas = np.zeros((4,len(lats),len(lons)))\n",
    "    mean_tracks_seas = np.zeros((4,len(lats),len(lons)))\n",
    "    mean_genesis_seas = np.zeros((4,len(lats),len(lons)))\n",
    "    mean_lysis_seas = np.zeros((4,len(lats),len(lons)))\n",
    "\n",
    "    #Loop over storm_tracks\n",
    "    #nr_storms = np.max(str_id)\n",
    "\n",
    "    if(distchar != \"Rossby\"):\n",
    "        dist_temp = dist_thresh\n",
    "\n",
    "    #Loop over storms\n",
    "    for strid in range(nrstorms): #clust_idxs: #range(1,nr_storms+1):\n",
    "        print(\"Storm: \" + str(strid))\n",
    "        if(connect):\n",
    "            temp_conn = str_connected[ids_storms[uniq_ids[strid]]]\n",
    "            temp_lat = str_lat[ids_storms[uniq_ids[strid]]][temp_conn >= 1]\n",
    "            temp_lon = str_lon[ids_storms[uniq_ids[strid]]][temp_conn >= 1]\n",
    "            temp_dt  = str_dt[ids_storms[uniq_ids[strid]]][temp_conn >= 1]\n",
    "        else:\n",
    "            temp_lat = str_lat[ids_storms[uniq_ids[strid]]]\n",
    "            temp_lon = str_lon[ids_storms[uniq_ids[strid]]]\n",
    "            temp_dt  = str_dt[ids_storms[uniq_ids[strid]]]\n",
    "        #temp_vort = str_vort[ids_storms[uniq_ids[strid]]]\n",
    "        #temp_maxvort = np.nanmax(str_vort[ids_storms[uniq_ids[strid]]])\n",
    "\n",
    "        lngth = len(temp_dt)\n",
    "\n",
    "        #Switch to prevent double counting\t\n",
    "        bool_tracks   = np.full((len(lats),len(lons)),False)\n",
    "\n",
    "        #Loop over times\n",
    "        for tridx in range(len(temp_dt)):\n",
    "            #print(\"Idx: \" + str(tridx))\n",
    "\n",
    "            #Find time index for current time of storm track in result array\n",
    "            if (temp_dt[tridx] in dt_array):\n",
    "                tidx = dt_array.index(temp_dt[tridx])\n",
    "                #Loop over lons and lats\n",
    "                for latidx in range(len(lats)):\n",
    "                    lattemp = np.abs(lats[latidx])\n",
    "                    if(distchar == \"Rossby\"):\n",
    "                        if(lattemp > 20):\n",
    "                            dist_temp = np.abs(calc_Rossby_radius(lat=lattemp))\n",
    "                        else:\n",
    "                            dist_temp = calc_Rossby_radius(lat=20.0)\n",
    "                    if(np.abs(temp_lat[tridx] - lats[latidx]) <= dist_temp/111): \n",
    "                        for lonidx in range(len(lons)):\n",
    "                            #Calculate distance to grid point\n",
    "                            dist = great_circle(temp_lat[tridx],temp_lon[tridx], lats[latidx],lons[lonidx])\n",
    "\n",
    "                            #If distance is < 500 km increase nr. of storms\n",
    "                            if ((dist < dist_temp)): \n",
    "                                storms[tidx,latidx,lonidx] += 1\n",
    "                                if(bool_tracks[latidx,lonidx] == False):\n",
    "                                    tracks[tidx,latidx,lonidx] += 1\n",
    "                                if(tridx == 0):\n",
    "                                    genesis[tidx,latidx,lonidx] += 1\n",
    "                                if(tridx == len(temp_dt) - 1):\t\n",
    "                                    lysis[tidx,latidx,lonidx] += 1\n",
    "\n",
    "                                bool_tracks[latidx,lonidx] = True   \n",
    "\n",
    "    if(distchar != \"Rossby\"):\n",
    "        mul_fac = (500/dist_thresh)**2.0*4.0/np.pi\n",
    "\n",
    "    #Loop over lons and lats\n",
    "    if(distchar ==\"Rossby\"):\n",
    "        mul_fac = np.zeros(conf.gridsize)\n",
    "        for latidx in range(len(lats)):\n",
    "            lattemp = np.abs(lats[latidx])\n",
    "            Rossby_temp = calc_Rossby_radius(lat=lattemp)\n",
    "            mul_fac[latidx,:] = (500/Rossby_temp)**2.0*4.0/np.pi\n",
    "\n",
    "    mean_storms = np.nanmean(storms,axis=0)*mul_fac\n",
    "    mean_tracks = np.nanmean(tracks,axis=0)*mul_fac\n",
    "    mean_genesis = np.nanmean(genesis,axis=0)*mul_fac\n",
    "    mean_lysis = np.nanmean(lysis,axis=0)*mul_fac\n",
    "    \n",
    "    ## seasonal differences ##\n",
    "    seasons = [\"DJF\",\"MAM\",\"JJA\",\"SON\"]\n",
    "    \n",
    "    i=0\n",
    "    for season in seasons:\n",
    "        months = np.array([x.month for x in dt_array])\n",
    "        if(season == \"DJF\"):\n",
    "            selidxs = (months < 3) | (months >= 12)\n",
    "        elif(season == \"MAM\"):\n",
    "            selidxs = (months < 6) | (months >= 3)\n",
    "        elif(season == \"JJA\"):\n",
    "            selidxs = (months < 9) | (months >= 6)\n",
    "        elif(season == \"SON\"):\n",
    "            selidxs = (months < 12) | (months >= 8)\n",
    "\n",
    "        mean_storms_seas[i,::] = np.nanmean(storms[selidxs,::],axis=0)\n",
    "        mean_tracks_seas[i,::] = np.nanmean(tracks[selidxs,::],axis=0)\n",
    "        mean_genesis_seas[i,::] = np.nanmean(genesis[selidxs,::],axis=0)\n",
    "        mean_lysis_seas[i,::]   = np.nanmean(lysis[selidxs,::],axis=0)\n",
    "        i+=1 \n",
    "        \n",
    "    ## Optionally saving results\n",
    "    if(save):\n",
    "        np.savez(outfile, \n",
    "        #storms=storms,\n",
    "        #tracks=tracks,\n",
    "        #genesis=genesis,\n",
    "        #lysis=lysis,\n",
    "        #yearly means\n",
    "        mean_storms=mean_storms,\n",
    "        mean_tracks=mean_tracks,\n",
    "        mean_genesis=mean_genesis,\n",
    "        mean_lysis=mean_lysis,\n",
    "        #Seasonal means\n",
    "        mean_storms_seas=mean_storms_seas,\n",
    "        mean_tracks_seas=mean_tracks_seas,\n",
    "        mean_genesis_seas=mean_genesis_seas,\n",
    "        mean_lysis_seas=mean_lysis_seas)\n",
    " \n",
    "    return mean_storms, mean_tracks, mean_genesis, mean_lysis, mean_storms_seas, mean_tracks_seas, mean_genesis_seas, mean_lysis_seas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae5bf221",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#ERA 5 Densities calculation  #######\n",
    "datachar = \"EI\"\n",
    "st_file_ei = \"Tracks/Selected_tracks_1979to2018_0101to1231_ei_Globe_Leonidas_with_stationary_all\"\n",
    "str_id, str_nr, str_dt, str_lat, str_lon = read_file(st_file_ei)\n",
    "str_dt = np.array(str_dt)\n",
    "\n",
    "#Save density\n",
    "outfile=\"Density/Density_\" + datachar + \"_\" + distchar + \".npz\"\n",
    "#np.savez(outfile, mean_storms=mean_storms_ei, mean_tracks=mean_tracks_ei,mean_genesis=mean_genesis_ei,mean_lysis=mean_lysis_ei)\n",
    "\n",
    "if(calcDensityEI):\n",
    "    #Density\n",
    "    mean_storms_ei, mean_tracks_ei, mean_genesis_ei, mean_lysis_ei, mean_storms_seas_ei, mean_tracks_seas_ei, mean_genesis_seas_ei, mean_lysis_seas_ei = calc_density_radius(save=True, outfile=outfile)\n",
    "else:\n",
    "    ResultsDensity = np.load(outfile)\n",
    "    mean_storms_ei = ResultsDensity[\"mean_storms\"]\n",
    "    mean_tracks_ei = ResultsDensity[\"mean_tracks\"]\n",
    "    mean_storms_seas_ei = ResultsDensity[\"mean_storms_seas\"]\n",
    "    mean_genesis_seas_ei = ResultsDensity[\"mean_genesis_seas\"]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68dedc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustchar = \"All\"\n",
    "minstorms = 2\n",
    "######################################################\n",
    "# load clustering results\n",
    "######################################################\n",
    "#infile = Options[\"outdir\"] +  Options[\"str_result\"]  + formatter.format( Options[\"distthresh\"]) + \"_tim_\" + formatter.format( Options[\"timthresh\"]) + \"_length_\" + formatter.format( Options[\"lngthresh\"]) + \".npz\"\n",
    "infile = Options[\"outdir\"] +  Options[\"str_result\"] + formatter.format( Options[\"distthresh\"]) + \"_tim_\" + formatter.format( Options[\"timthresh\"]) + \"_length_\" + formatter.format( Options[\"lngthresh\"]) + \"_timlength_\" + formatter.format( Options[\"timlngthresh\"]*6.0) + \"_\" + Options[\"distmeth\"] + \".npz\"\n",
    "\n",
    "Results = np.load(infile,allow_pickle=True)\n",
    " \n",
    "# All clusters\n",
    "sorted_clusters = Results[\"sorted_clusters\"]\n",
    "\n",
    "# Only Subclusters (length or nolength)\n",
    "sorted_subclusters = Results[\"sorted_subclusters_nolength\"]\n",
    "sorted_clusters_nolength = sorted(unnest(sorted_subclusters))\n",
    "sorted_subclusters = Results[\"sorted_subclusters_length\"]\n",
    "sorted_clusters_length = sorted(unnest(sorted_subclusters))\n",
    "\n",
    "#Get str_connected\n",
    "str_connected = Results[\"str_connected\"]\n",
    "\n",
    "#Filter clusterd storms\n",
    "strmidxs = np.unique(str_id)\n",
    "clststroms = [strm for cluster in sorted_clusters for strm in cluster if len(cluster) >= minstorms and strm in strmidxs]\n",
    "clststroms = sorted(clststroms)\n",
    "len(clststroms)\n",
    "len(clststroms)/np.nanmax(str_id)\n",
    "######################################################\n",
    "# load density results All\n",
    "######################################################\n",
    "#outfileClust =\"Density/Density_\" +\\\n",
    "#        datachar + \"_\" + distchar + \"_Clust_\" + clustchar + \"_minstorms_\" + str(minstorms) + \"_connect.npz\"\n",
    "#outfileClust=\"Density/Density_\" + \"_\" + distchar + \"_\" +\\\n",
    "#        Options[\"str_result\"]  + formatter.format( Options[\"distthresh\"]) + \"_tim_\" + formatter.format( Options[\"timthresh\"]) + \"_length_\" + formatter.format( Options[\"lngthresh\"    ]) +\\\n",
    "#        \"_Clust_\" + clustchar + \"_minstorms_\" + str(minstorms) + \"_connect.npz\"\n",
    "\n",
    "outfileClust=\"Density/Density_\" + \"_\" + distchar + \"_\" +\\\n",
    "            Options[\"str_result\"]  + formatter.format( Options[\"distthresh\"]) + \"_tim_\" + formatter.format( Options[\"timthresh\"]) + \"_length_\" + formatter.format( Options[\"lngthresh\"    ]) +\\\n",
    "    \"_timlength_\" + formatter.format( Options[\"timlngthresh\"]*6.0) + \"_\" + Options[\"distmeth\"] +\\\n",
    "            \"_Clust_\" + clustchar + \"_minstorms_\" + str(minstorms) + \"_connect.npz\"\n",
    "\n",
    "ResultsDensityClust = np.load(outfileClust)\n",
    "mean_storms_clst_ei = ResultsDensityClust[\"mean_storms\"]\n",
    "mean_tracks_clst_ei = ResultsDensityClust[\"mean_tracks\"]\n",
    "mean_storms_seas_clst_ei = ResultsDensityClust[\"mean_storms_seas\"]\n",
    "mean_genesis_clst_ei = ResultsDensityClust[\"mean_genesis_seas\"]\n",
    "\n",
    "######################################################\n",
    "# load density results length\n",
    "######################################################\n",
    "clustchar = \"length\"\n",
    "minstorms = 2\n",
    "\n",
    "#outfileClust=\"Density/Density_\" + \"_\" + distchar + \"_\" +\\\n",
    "#        Options[\"str_result\"]  + formatter.format( Options[\"distthresh\"]) + \"_tim_\" + formatter.format( Options[\"timthresh\"]) + \"_length_\" + formatter.format( Options[\"lngthresh\"    ]) +\\\n",
    "#        \"_Clust_\" + clustchar + \"_minstorms_\" + str(minstorms) + \"_connect.npz\"\n",
    "outfileClust=\"Density/Density_\" + \"_\" + distchar + \"_\" +\\\n",
    "            Options[\"str_result\"]  + formatter.format( Options[\"distthresh\"]) + \"_tim_\" + formatter.format( Options[\"timthresh\"]) + \"_length_\" + formatter.format( Options[\"lngthresh\"    ]) +\\\n",
    "    \"_timlength_\" + formatter.format( Options[\"timlngthresh\"]*6.0) + \"_\" + Options[\"distmeth\"] +\\\n",
    "            \"_Clust_\" + clustchar + \"_minstorms_\" + str(minstorms) + \"_connect.npz\"\n",
    "\n",
    "ResultsDensityClust_length = np.load(outfileClust)\n",
    "mean_storms_clst_ei_length = ResultsDensityClust_length[\"mean_storms\"]\n",
    "mean_tracks_clst_ei_length = ResultsDensityClust_length[\"mean_tracks\"]\n",
    "mean_storms_seas_clst_ei_length = ResultsDensityClust_length[\"mean_storms_seas\"]\n",
    "mean_genesis_clst_ei_length = ResultsDensityClust_length[\"mean_genesis_seas\"]\n",
    "\n",
    "######################################################\n",
    "# load density results nolength\n",
    "######################################################\n",
    "clustchar = \"nolength\"\n",
    "\n",
    "#outfileClust=\"Density/Density_\" + \"_\" + distchar + \"_\" +\\\n",
    "#        Options[\"str_result\"]  + formatter.format( Options[\"distthresh\"]) + \"_tim_\" + formatter.format( Options[\"timthresh\"]) + \"_length_\" + formatter.format( Options[\"lngthresh\"    ]) +\\\n",
    "#        \"_Clust_\" + clustchar + \"_minstorms_\" + str(minstorms) + \"_connect.npz\"\n",
    "outfileClust=\"Density/Density_\" + \"_\" + distchar + \"_\" +\\\n",
    "            Options[\"str_result\"]  + formatter.format( Options[\"distthresh\"]) + \"_tim_\" + formatter.format( Options[\"timthresh\"]) + \"_length_\" + formatter.format( Options[\"lngthresh\"    ]) +\\\n",
    "    \"_timlength_\" + formatter.format( Options[\"timlngthresh\"]*6.0) + \"_\" + Options[\"distmeth\"] +\\\n",
    "            \"_Clust_\" + clustchar + \"_minstorms_\" + str(minstorms) + \"_connect.npz\"\n",
    "\n",
    "\n",
    "ResultsDensityClust_nolength = np.load(outfileClust)\n",
    "mean_storms_clst_ei_nolength = ResultsDensityClust_nolength[\"mean_storms\"]\n",
    "mean_tracks_clst_ei_nolength = ResultsDensityClust_nolength[\"mean_tracks\"]\n",
    "mean_storms_seas_clst_ei_nolength = ResultsDensityClust_nolength[\"mean_storms_seas\"]\n",
    "mean_genesis_clst_ei_nolength = ResultsDensityClust_nolength[\"mean_genesis_seas\"]\n",
    "\n",
    "######################################################\n",
    "# load other results (sparse matrices)\n",
    "######################################################\n",
    "from scipy.sparse import load_npz\n",
    "infile = Options[\"outdir\"] +  Options[\"str_result\"]  + formatter.format( Options[\"distthresh\"]) + \"_tim_\" + formatter.format( Options[\"timthresh\"]) + \"_length_\" + formatter.format( Options[\"lngthresh\"]) \n",
    "dtTracks = load_npz(infile + \"_dtTracks.npz\")\n",
    "drTracks = load_npz(infile + \"_drTracks.npz\")\n",
    "connTracks = load_npz(infile + \"_connTracks.npz\")\n",
    "\n",
    "#Convert to dok_matrix to make subscriptable\n",
    "connTracks = connTracks.todok()\n",
    "dtTracks = dtTracks.todok()\n",
    "drTracks = drTracks.todok()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dynpie3_updated",
   "language": "python",
   "name": "dynpie3_updated"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
